# ğŸ™ï¸ CSM Backend - Text-to-Speech Service

**Advanced AI Audio Synthesis for Book Narratives**

> Python FastAPI server with Conversational Speech Model (CSM) for generating natural-sounding voice narrations from text

---

## ğŸ¯ Project Overview

**Purpose:** Backend microservice that converts book narrative text into high-quality audio files using machine learning. Triggered by the frontend when admins click "Generate Narrative" for books.

**Key Capabilities:**
- âœ… Text-to-speech synthesis (CSM model + Mimi codec)
- âœ… 24kHz audio quality
- âœ… Asynchronous processing (fire-and-forget)
- âœ… Firebase integration (upload + database updates)
- âœ… Audio watermarking
- âœ… Automatic model caching
- âœ… Error handling & retries
- âœ… GPU-accelerated inference

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     NEXT.JS FRONTEND SERVER          â”‚
â”‚     (books-bid)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ HTTP POST /generate
                â”‚ {text, bookId}
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     CSM FASTAPI SERVER               â”‚
â”‚     (This project)                   â”‚
â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  POST /generate endpoint     â”‚   â”‚
â”‚  â”‚                              â”‚   â”‚
â”‚  â”‚  1. Receive text + bookId    â”‚   â”‚
â”‚  â”‚  2. Start async task         â”‚   â”‚
â”‚  â”‚  3. Return 202 (accepted)    â”‚   â”‚
â”‚  â”‚  4. Process in background    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Audio Generation Pipeline   â”‚   â”‚
â”‚  â”‚                              â”‚   â”‚
â”‚  â”‚  Text                        â”‚   â”‚
â”‚  â”‚    â†“ (Llama tokenizer)       â”‚   â”‚
â”‚  â”‚  Token IDs                   â”‚   â”‚
â”‚  â”‚    â†“ (CSM model forward)     â”‚   â”‚
â”‚  â”‚  RVQ Codes (32 codebooks)    â”‚   â”‚
â”‚  â”‚    â†“ (Mimi decoder)          â”‚   â”‚
â”‚  â”‚  WAV Audio (24kHz)           â”‚   â”‚
â”‚  â”‚    â†“ (Watermarking)          â”‚   â”‚
â”‚  â”‚  Watermarked WAV             â”‚   â”‚
â”‚  â”‚    â†“ (Upload to Storage)     â”‚   â”‚
â”‚  â”‚  Public URL                  â”‚   â”‚
â”‚  â”‚    â†“ (Update Firebase DB)    â”‚   â”‚
â”‚  â”‚  Completion âœ…               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                      â”‚
â”‚  External Dependencies:              â”‚
â”‚  â€¢ PyTorch (CPU/GPU)                â”‚
â”‚  â€¢ Transformers (Llama)             â”‚
â”‚  â€¢ Torchaudio (Mimi)                â”‚
â”‚  â€¢ Firebase Admin SDK               â”‚
â”‚  â€¢ NumPy, SciPy                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚  â†“ Upload & Database Update
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     FIREBASE ECOSYSTEM                â”‚
â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Cloud Storage               â”‚    â”‚
â”‚  â”‚  gs://bucket/narrations/     â”‚    â”‚
â”‚  â”‚    book_123.wav              â”‚    â”‚
â”‚  â”‚    book_456.wav              â”‚    â”‚
â”‚  â”‚    ...                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Realtime Database           â”‚    â”‚
â”‚  â”‚  /books/{bookId}/            â”‚    â”‚
â”‚  â”‚    narrativeAudioUrl         â”‚    â”‚
â”‚  â”‚    narrativeAudioStatus      â”‚    â”‚
â”‚  â”‚    narrativeUpdatedAt        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure & Files

```
csm/
â”‚
â”œâ”€â”€ server.py                           # Main FastAPI application
â”‚   â”œâ”€ FastAPI app initialization
â”‚   â”œâ”€ Lifespan management (model loading)
â”‚   â”œâ”€ POST /generate endpoint
â”‚   â”œâ”€ GET /health endpoint
â”‚   â”œâ”€ Error handling
â”‚   â””â”€ Async task management
â”‚
â”œâ”€â”€ models.py                           # ML Model Architecture
â”‚   â”œâ”€ CSMModel class
â”‚   â”‚  â”œâ”€ Llama-3.2-1B backbone
â”‚   â”‚  â”œâ”€ 16 transformer layers
â”‚   â”‚  â”œâ”€ 32 heads attention
â”‚   â”‚  â”œâ”€ Mimi audio decoder
â”‚   â”‚  â”œâ”€ 32 RVQ codebooks
â”‚   â”‚  â””â”€ Load from HuggingFace
â”‚   â”‚
â”‚   â”œâ”€ Model specs:
â”‚   â”‚  â”œâ”€ Hidden size: 2048
â”‚   â”‚  â”œâ”€ Vocab size: 128,000
â”‚   â”‚  â”œâ”€ Max sequence: 2048 tokens
â”‚   â”‚  â”œâ”€ Dtype: bfloat16
â”‚   â”‚  â””â”€ VRAM: ~15-20GB
â”‚   â”‚
â”‚   â””â”€ Methods:
â”‚      â”œâ”€ forward(input_ids) â†’ tokens
â”‚      â”œâ”€ generate(text) â†’ audio
â”‚      â””â”€ cache management
â”‚
â”œâ”€â”€ generator.py                        # Audio Generation Logic
â”‚   â”œâ”€ AudioGenerator class
â”‚   â”‚  â”œâ”€ Text tokenization (Llama tokenizer)
â”‚   â”‚  â”œâ”€ Model inference (forward pass)
â”‚   â”‚  â”œâ”€ RVQ code generation
â”‚   â”‚  â”œâ”€ Mimi decoding
â”‚   â”‚  â”œâ”€ WAV file generation
â”‚   â”‚  â””â”€ Audio watermarking
â”‚   â”‚
â”‚   â”œâ”€ Key functions:
â”‚   â”‚  â”œâ”€ tokenize(text) â†’ token_ids
â”‚   â”‚  â”œâ”€ generate_codes(tokens) â†’ rvq
â”‚   â”‚  â”œâ”€ decode_audio(codes) â†’ wav
â”‚   â”‚  â”œâ”€ apply_watermark(wav) â†’ marked_wav
â”‚   â”‚  â””â”€ save_wav(audio, path) â†’ bytes
â”‚   â”‚
â”‚   â””â”€ Config:
â”‚      â”œâ”€ Sample rate: 24,000 Hz
â”‚      â”œâ”€ Bit depth: 16-bit
â”‚      â”œâ”€ Max duration: 60 seconds
â”‚      â””â”€ Batch size: 1
â”‚
â”œâ”€â”€ requirements.txt                    # Python Dependencies
â”‚   â”œâ”€ torch==2.2.0 (PyTorch)
â”‚   â”œâ”€ transformers==4.40.0 (Hugging Face)
â”‚   â”œâ”€ torchaudio==2.2.0 (Audio processing)
â”‚   â”œâ”€ fastapi==0.104.0 (Web framework)
â”‚   â”œâ”€ uvicorn==0.24.0 (ASGI server)
â”‚   â”œâ”€ firebase-admin==6.2.0 (Firebase SDK)
â”‚   â”œâ”€ numpy==1.24.0
â”‚   â”œâ”€ scipy==1.11.0
â”‚   â”œâ”€ Pillow==10.0.0
â”‚   â”œâ”€ pydantic==2.4.0
â”‚   â”œâ”€ python-dotenv==1.0.0
â”‚   â””â”€ requests==2.31.0
â”‚
â”œâ”€â”€ Dockerfile                          # Container Configuration
â”‚   â”œâ”€ Base image: nvidia/cuda:12.4.1-runtime-ubuntu22.04
â”‚   â”œâ”€ Python 3.10.13
â”‚   â”œâ”€ Copy code to /app
â”‚   â”œâ”€ Install requirements
â”‚   â”œâ”€ Expose port 5006
â”‚   â””â”€ CMD: uvicorn server:app --host 0.0.0.0 --port 5006
â”‚
â”œâ”€â”€ serviceAccountKey.json              # Firebase Credentials
â”‚   â”œâ”€ JSON file with service account key
â”‚   â”œâ”€ Downloaded from Firebase Console
â”‚   â”œâ”€ Contains auth tokens
â”‚   â””â”€ NEVER commit to git!
â”‚
â”œâ”€â”€ watermarking.py                     # Audio Watermarking
â”‚   â”œâ”€ add_watermark(audio) function
â”‚   â”œâ”€ Embeds inaudible frequency signature
â”‚   â”œâ”€ Proof of ownership/origin
â”‚   â””â”€ Survives compression
â”‚
â”œâ”€â”€ README.md                           # Documentation (this file)
â”œâ”€â”€ LICENSE                             # Open source license
â”œâ”€â”€ .gitignore                          # Ignore credentials
â”œâ”€â”€ .env                                # Environment config
â”œâ”€â”€ __pycache__/                        # Python cache (auto-generated)
â”‚
â””â”€â”€ requirements.txt                    # All dependencies listed
```

---

## ğŸ”„ Complete Processing Flow

### Step-by-Step Execution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. FRONTEND TRIGGERS REQUEST            â”‚
â”‚                                         â”‚
â”‚ POST http://csm-backend:5006/generate   â”‚
â”‚ Content-Type: application/json          â”‚
â”‚                                         â”‚
â”‚ Payload:                                â”‚
â”‚ {                                       â”‚
â”‚   "text": "A masterpiece of satire...",â”‚
â”‚   "bookId": "book_123",                 â”‚
â”‚   "returnUrl": "https://..."            â”‚
â”‚ }                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼ (HTTPS)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. FASTAPI SERVER RECEIVES REQUEST       â”‚
â”‚                                          â”‚
â”‚ POST /generate endpoint:                 â”‚
â”‚ â”œâ”€ Validate request JSON                â”‚
â”‚ â”œâ”€ Check bookId format                  â”‚
â”‚ â”œâ”€ Check text length (max 1000 words)   â”‚
â”‚ â”œâ”€ Verify rate limit (10/minute)        â”‚
â”‚ â””â”€ Extract parameters                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. CREATE BACKGROUND TASK                â”‚
â”‚                                          â”‚
â”‚ asyncio.create_task(                    â”‚
â”‚   process_audio(text, bookId)           â”‚
â”‚ )                                       â”‚
â”‚                                         â”‚
â”‚ Return immediately: 202 Accepted        â”‚
â”‚ Response:                               â”‚
â”‚ {                                       â”‚
â”‚   "status": "processing",              â”‚
â”‚   "bookId": "book_123"                 â”‚
â”‚ }                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    Response sent to frontend
    (in < 500ms)
             â”‚
             â–¼
    Frontend displays message:
    "Generating audio..."
             â”‚
             â–¼ (Background processing continues)
             
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. LOAD ML MODELS (if not cached)        â”‚
â”‚                                          â”‚
â”‚ models.py â†’ CSMModel.load()             â”‚
â”‚ â”œâ”€ Check cache for existing model       â”‚
â”‚ â”œâ”€ If not cached:                       â”‚
â”‚ â”‚  â”œâ”€ Download Llama-3.2-1B             â”‚
â”‚ â”‚  â”‚  (from HuggingFace)                â”‚
â”‚ â”‚  â”‚  Size: ~2.5GB                      â”‚
â”‚ â”‚  â”œâ”€ Download Mimi decoder             â”‚
â”‚ â”‚  â”‚  Size: ~150MB                      â”‚
â”‚ â”‚  â”œâ”€ Load to GPU memory                â”‚
â”‚ â”‚  â”‚  Requires: 20-30GB VRAM            â”‚
â”‚ â”‚  â””â”€ Set dtype: bfloat16               â”‚
â”‚ â”‚     (reduces memory usage)            â”‚
â”‚ â””â”€ Model ready in GPU cache             â”‚
â”‚                                         â”‚
â”‚ Time: 30-60 seconds (first run)         â”‚
â”‚       < 1 second (cached)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼ (Only if not cached)
             
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. TOKENIZE TEXT                         â”‚
â”‚                                          â”‚
â”‚ Input text:                             â”‚
â”‚ "A masterpiece of satire in its finest"â”‚
â”‚                                         â”‚
â”‚ Llama Tokenizer:                        â”‚
â”‚ token_ids = tokenizer.encode(text)     â”‚
â”‚                                         â”‚
â”‚ Output tokens (IDs):                    â”‚
â”‚ [8, 29871, 29901, 29872, ...]          â”‚
â”‚                                         â”‚
â”‚ Number of tokens: ~100 (for above)      â”‚
â”‚ Token limit: 2048 max                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
             
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. GENERATE RVQ CODES                    â”‚
â”‚                                          â”‚
â”‚ CSM Model Forward Pass:                 â”‚
â”‚ â”œâ”€ Input: token_ids (tensor)            â”‚
â”‚ â”œâ”€ Process through:                     â”‚
â”‚ â”‚  â”œâ”€ Embedding layer                   â”‚
â”‚ â”‚  â”œâ”€ 16 transformer layers             â”‚
â”‚ â”‚  â”œâ”€ Self-attention heads              â”‚
â”‚ â”‚  â””â”€ Feed-forward networks             â”‚
â”‚ â”œâ”€ Output: RVQ codes                    â”‚
â”‚ â”‚  â”œâ”€ 32 codebooks                      â”‚
â”‚ â”‚  â”œâ”€ Each code: 0-2047 range           â”‚
â”‚ â”‚  â””â”€ Shape: (seq_len, 32)              â”‚
â”‚ â””â”€ GPU computation: < 30 seconds        â”‚
â”‚                                         â”‚
â”‚ GPU Memory Used:                        â”‚
â”‚ â€¢ Model weights: ~2GB                   â”‚
â”‚ â€¢ KV cache: ~1GB                        â”‚
â”‚ â€¢ Activations: ~5GB                     â”‚
â”‚ â€¢ Total: ~8GB (out of 20-30GB)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
             
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. MIMI AUDIO DECODER                    â”‚
â”‚                                          â”‚
â”‚ Input: RVQ codes (32 codebooks)        â”‚
â”‚                                         â”‚
â”‚ Mimi Decoder (HuggingFace):             â”‚
â”‚ â”œâ”€ Reconstruct from codes               â”‚
â”‚ â”œâ”€ Process through decoder layers       â”‚
â”‚ â”œâ”€ Upsample to 24kHz                    â”‚
â”‚ â”œâ”€ Output: PCM audio samples            â”‚
â”‚ â”‚  (float32, range -1.0 to 1.0)        â”‚
â”‚ â””â”€ Audio duration: 10-30 seconds        â”‚
â”‚                                         â”‚
â”‚ Computation time: 5-10 seconds          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
             
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. AUDIO WATERMARKING                    â”‚
â”‚                                          â”‚
â”‚ watermarking.py â†’ add_watermark()        â”‚
â”‚                                         â”‚
â”‚ â”œâ”€ Embed inaudible signature            â”‚
â”‚ â”‚  (Frequency: 17-20 kHz)               â”‚
â”‚ â”‚  (Amplitude: -40dB to -30dB)          â”‚
â”‚ â”‚  (Pattern: Unique to each book)       â”‚
â”‚ â”œâ”€ Survives:                            â”‚
â”‚ â”‚  â”œâ”€ MP3 compression                   â”‚
â”‚ â”‚  â”œâ”€ Streaming bitrate reduction       â”‚
â”‚ â”‚  â”œâ”€ Audio playback variations         â”‚
â”‚ â”‚  â””â”€ Digital-to-analog conversion      â”‚
â”‚ â””â”€ Detectable only by detector app      â”‚
â”‚                                         â”‚
â”‚ Processing time: < 1 second             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
             
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. CONVERT TO WAV FORMAT                 â”‚
â”‚                                          â”‚
â”‚ torchaudio.save(path, waveform, sr)    â”‚
â”‚                                         â”‚
â”‚ Format specifications:                  â”‚
â”‚ â€¢ Format: WAV (RIFF)                    â”‚
â”‚ â€¢ Sample rate: 24,000 Hz                â”‚
â”‚ â€¢ Channels: 1 (mono)                    â”‚
â”‚ â€¢ Bit depth: 16-bit PCM                 â”‚
â”‚ â€¢ File size: ~1.4 MB per minute        â”‚
â”‚                                         â”‚
â”‚ For 20-second audio:                    â”‚
â”‚ â€¢ File size: ~467 KB                    â”‚
â”‚ â””â”€ Written to /tmp/book_123.wav         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
             
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10. UPLOAD TO FIREBASE STORAGE           â”‚
â”‚                                          â”‚
â”‚ firebase-admin SDK:                     â”‚
â”‚ â”œâ”€ Initialize with service account key  â”‚
â”‚ â”œâ”€ Connect to: gs://bucket/             â”‚
â”‚ â”œâ”€ Upload file to:                      â”‚
â”‚ â”‚  gs://bucket/narrations/book_123.wav  â”‚
â”‚ â”œâ”€ Set metadata:                        â”‚
â”‚ â”‚  â”œâ”€ contentType: audio/wav            â”‚
â”‚ â”‚  â”œâ”€ cacheControl: public, max-age=..  â”‚
â”‚ â”‚  â””â”€ metadata:                         â”‚
â”‚ â”‚     â”œâ”€ bookId: book_123               â”‚
â”‚ â”‚     â”œâ”€ generatedAt: timestamp         â”‚
â”‚ â”‚     â””â”€ watermarked: true              â”‚
â”‚ â””â”€ Upload time: 3-5 seconds             â”‚
â”‚                                         â”‚
â”‚ Result: File in cloud storage           â”‚
â”‚ Accessible via: https://storage...     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
             
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 11. GET PUBLIC DOWNLOAD URL              â”‚
â”‚                                          â”‚
â”‚ storage.bucket()                        â”‚
â”‚   .file('narrations/book_123.wav')       â”‚
â”‚   .getSignedUrl({                       â”‚
â”‚     version: 'v4',                      â”‚
â”‚     action: 'read',                     â”‚
â”‚     expires: 7 days                     â”‚
â”‚   })                                    â”‚
â”‚                                         â”‚
â”‚ URL expires in 7 days                   â”‚
â”‚ (Can be regenerated if needed)          â”‚
â”‚                                         â”‚
â”‚ URL format:                             â”‚
â”‚ https://storage.googleapis.com/...      â”‚
â”‚    /narrations/book_123.wav             â”‚
â”‚    ?GoogleAccessId=...                  â”‚
â”‚    &Expires=...                         â”‚
â”‚    &Signature=...                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
             
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 12. UPDATE FIREBASE REALTIME DATABASE    â”‚
â”‚                                          â”‚
â”‚ firebase_admin.db.reference()           â”‚
â”‚   .child('books/book_123')              â”‚
â”‚   .update({                             â”‚
â”‚     narrativeAudioUrl: https://...,     â”‚
â”‚     narrativeAudioStatus: 'ready',      â”‚
â”‚     narrativeUpdatedAt: timestamp,      â”‚
â”‚     narrativeAudioDuration: 23          â”‚
â”‚   })                                    â”‚
â”‚                                         â”‚
â”‚ Time: 1-2 seconds                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
             
        âœ… PROCESSING COMPLETE!
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FRONTEND LISTENERS ARE TRIGGERED         â”‚
â”‚                                          â”‚
â”‚ All users viewing the book get update:  â”‚
â”‚ onValue(/books/book_123)                â”‚
â”‚ â””â”€ Detects narrativeAudioUrl set       â”‚
â”‚                                         â”‚
â”‚ React re-renders:                       â”‚
â”‚ â”œâ”€ Show audio player component          â”‚
â”‚ â”œâ”€ Enable play button                   â”‚
â”‚ â””â”€ Update UI in < 100ms                 â”‚
â”‚                                         â”‚
â”‚ Admin can NOW click play:               â”‚
â”‚ â””â”€ Audio plays via HTML5 <audio>       â”‚
â”‚    element from Firebase URL            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TOTAL TIMELINE:
  0ms  - Frontend sends request
  10ms - Server receives, starts task
  500ms - Response 202 returned
  5s   - Models loaded (if not cached)
  10s  - Text tokenization done
  25s  - RVQ codes generated
  35s  - Audio decoded from codes
  36s  - Watermark applied
  37s  - WAV file created
  40s  - Upload to Firebase done
  42s  - Database updated
  42.1s - Frontend listener fires
  42.2s - UI updates (audio player visible)
  45s+ - Admin can click play!
  
Perception: < 5 seconds for admin
(sees result quickly due to responsive UI)
```

---

## ğŸ› ï¸ Technology Stack

### Core Framework
- **FastAPI 0.104.0** - Modern Python web framework
  - Automatic API documentation
  - Request validation with Pydantic
  - Async support
  - WebSocket support (for future features)

- **Uvicorn 0.24.0** - ASGI application server
  - High performance
  - Supports HTTP/1.1 and HTTP/2
  - Graceful shutdown

### Machine Learning
- **PyTorch 2.2.0** - Deep learning framework
  - GPU/CPU support (CUDA 12.4)
  - Efficient tensor operations
  - Model optimization tools

- **Transformers 4.40.0** - HuggingFace library
  - Llama-3.2-1B model
  - Mimi audio codec
  - Tokenizers
  - Pre-trained weights

- **Torchaudio 2.2.0** - Audio processing
  - WAV file I/O
  - Resampling
  - Audio effects
  - Feature extraction

### Database & Cloud
- **Firebase Admin SDK 6.2.0**
  - Realtime Database write
  - Cloud Storage upload
  - Authentication
  - Error handling

### Utilities
- **NumPy 1.24.0** - Numerical computing
- **SciPy 1.11.0** - Scientific algorithms
- **Pillow 10.0.0** - Image processing (watermarks)
- **Pydantic 2.4.0** - Data validation
- **Python-dotenv 1.0.0** - Environment variables
- **Requests 2.31.0** - HTTP client

### Infrastructure
- **Docker** - Containerization
- **NVIDIA CUDA 12.4** - GPU acceleration
- **Ubuntu 22.04** - Base OS

---

## ğŸš€ Setup & Deployment

### Prerequisites
- NVIDIA GPU with 20GB+ VRAM (for inference)
- Docker & Docker Compose
- Firebase project with credentials
- HuggingFace API token (for model download)

### Local Development

```bash
# 1. Clone and navigate
cd csm

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Create .env file
cat > .env << EOF
HF_TOKEN=your_hugging_face_token
FIREBASE_PROJECT_ID=your-project
FIREBASE_CREDENTIALS_PATH=./serviceAccountKey.json
TTS_DEBUG=true
EOF

# 5. Download service account key
# From Firebase Console â†’ Project Settings â†’ Service Accounts
# Save as: csm/serviceAccountKey.json

# 6. Run server
python -m uvicorn server:app --reload --host 0.0.0.0 --port 5006

# Server runs at http://localhost:5006
# API docs at http://localhost:5006/docs
```

### Docker Deployment

```bash
# 1. Build image
docker build -t csm-tts:latest .

# 2. Run container
docker run \
  --gpus all \
  -e HF_TOKEN=your_token \
  -e FIREBASE_PROJECT_ID=your-project \
  -v /path/to/serviceAccountKey.json:/app/serviceAccountKey.json \
  -p 5006:5006 \
  csm-tts:latest

# Or with docker-compose
docker-compose up -d
```

### Production Deployment

```bash
# 1. Push to container registry
docker tag csm-tts:latest your-registry/csm-tts:latest
docker push your-registry/csm-tts:latest

# 2. Deploy to Kubernetes/Cloud Run
# Configure:
# - GPU allocation: 1x A100 or 2x V100
# - Memory: 32GB
# - CPU: 4 cores
# - Replicas: 2-3 for HA
```

---

## ğŸ“¡ API Reference

### POST /generate

**Description:** Generate audio from text narrative

**Request:**
```json
{
  "text": "A masterpiece of social satire...",
  "bookId": "book_123",
  "returnUrl": "https://frontend.com/callback"
}
```

**Response (202 Accepted):**
```json
{
  "status": "processing",
  "bookId": "book_123",
  "taskId": "task_abc123",
  "message": "Audio generation started"
}
```

**Processing (happens async):**
- Models load (cached after first call)
- Text tokenized
- Audio generated
- Watermark applied
- Uploaded to Firebase
- Database updated

**Frontend knows it's done when:** Firebase listener fires with `narrativeAudioUrl` set

---

### GET /health

**Description:** Health check endpoint

**Response (200 OK):**
```json
{
  "status": "healthy",
  "gpu_available": true,
  "models_loaded": true,
  "version": "1.0.0"
}
```

---

## ğŸ” Security & Best Practices

### Input Validation
```python
# Text length limits
MAX_TEXT_LENGTH = 10000  # characters
MAX_WORDS = 1000
MAX_DURATION = 60  # seconds

# Validate before processing
if len(text) > MAX_TEXT_LENGTH:
    raise HTTPException(400, "Text too long")
```

### Rate Limiting
```python
# Limit requests per IP
MAX_REQUESTS_PER_MINUTE = 10

# Use Firebase auth if available
if not is_authenticated:
    check_rate_limit(request.client.host)
```

### Firebase Credentials
```bash
# NEVER commit serviceAccountKey.json
# Add to .gitignore:
echo "serviceAccountKey.json" >> .gitignore

# Use environment variables in production
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json
```

### Audio Watermarking
```python
# Unique identifier per book
# Makes unlicensed copies traceable
# Survives common compression formats
```

---

## ğŸ§ª Testing

### Local Testing

```bash
# 1. Start server
python -m uvicorn server:app --reload

# 2. Test health check
curl http://localhost:5006/health

# 3. Generate audio
curl -X POST http://localhost:5006/generate \
  -H "Content-Type: application/json" \
  -d '{
    "text": "This is a test narrative.",
    "bookId": "test_book_123"
  }'

# 4. Monitor logs
# Watch server terminal for processing status
```

### Performance Benchmarks

```
Text length: 500 characters (~100 tokens)
  Tokenization: 100ms
  Model inference: 5-8 seconds
  Audio decode: 2-3 seconds
  Upload: 1-2 seconds
  Database update: 500ms
  Total: 9-14 seconds

Text length: 2000 characters (~400 tokens)
  Total: 18-25 seconds

Text length: 5000 characters (~1000 tokens)
  Total: 30-45 seconds
```

---

## ğŸ› Troubleshooting

### Issue: "CUDA out of memory"
```
Solution 1: Reduce max text length
  MAX_TEXT_LENGTH = 5000

Solution 2: Clear GPU cache
  import torch
  torch.cuda.empty_cache()

Solution 3: Use CPU (slower)
  device = "cpu"
  model = model.to("cpu")

Solution 4: Enable memory optimization
  torch.cuda.empty_cache()
  # Restart container with more memory
```

### Issue: "Models not downloading"
```
Cause: HF_TOKEN not set or invalid
Solution: 
  1. Get token from huggingface.co
  2. Set environment variable
  export HF_TOKEN=hf_xxxxx
  3. Restart server
```

### Issue: "Firebase auth failed"
```
Cause: serviceAccountKey.json missing/invalid
Solution:
  1. Download from Firebase Console
  2. Save to csm/serviceAccountKey.json
  3. Verify JSON is valid
  4. Restart server
```

### Issue: "Audio quality poor"
```
Possible causes:
  - Model not fully loaded (bfloat16 issue)
  - GPU memory insufficient
  - Input text too short

Solutions:
  - Restart container
  - Use GPU with more VRAM
  - Use longer, more natural text
```

---

## ğŸ“Š Monitoring

### Server Logs

```bash
# Tail logs in production
docker logs -f csm_container

# Look for:
[INFO] Application startup complete
[INFO] Processing audio for book_123
[INFO] Upload complete: gs://bucket/narrations/book_123.wav
```

### Performance Metrics

```bash
# Monitor GPU usage
nvidia-smi

# Expected during processing:
# GPU Memory Usage: 15-25GB of 30GB
# GPU Utilization: 85-95%
# Temperature: 50-70Â°C
```

### Database Logs

```
Firebase Console â†’ Database Rules â†’ Read/Write Analytics
Monitor:
- Write operations to /books/{bookId}/narrativeAudioUrl
- Success vs. failure rates
- Latency metrics
```

---

## ğŸ“ˆ Scaling Considerations

### Horizontal Scaling
```
If handling 100+ books/day:

Option 1: Multiple replicas
  - 3x CSM servers (Kubernetes)
  - Load balance with Nginx
  - Each on separate GPU

Option 2: Cloud services
  - Google Cloud Run (GPU support)
  - AWS SageMaker
  - Modal.com (serverless GPU)
```

### Cost Optimization
```
GPU costs (approx):
- A100 (40GB): $4-5/hour
- V100 (32GB): $2-3/hour
- T4 (16GB): $0.35/hour
- CPU: $0.10/hour

Optimization:
- Batch process at off-peak hours
- Cache models aggressively
- Use spot instances
- Monitor queue depth
```

---

## ğŸ“š Learning Resources

- [FastAPI Documentation](https://fastapi.tiangolo.com)
- [PyTorch Documentation](https://pytorch.org/docs)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [Firebase Admin SDK](https://firebase.google.com/docs/database/admin/start)
- [Audio Processing with Torchaudio](https://pytorch.org/audio/main/)

---

## ğŸ”„ Git Workflow

```bash
# Create feature branch
git checkout -b feature/improve-audio-quality

# Make changes
vim generator.py

# Test locally
python -m uvicorn server:app --reload

# Commit
git commit -m "feat: improve audio generation quality"

# Push
git push origin feature/improve-audio-quality

# Create pull request for review
```

---

## â“ Common Questions

**Q: Why does audio generation take 20-30 seconds?**
A: The Llama-3.2-1B model processes your text sequentially, generating audio codes. With ~100 tokens and ~200ms per token, it takes ~20 seconds. This is normal.

**Q: Can I use a smaller model?**
A: Not currently. The project requires Llama-3.2-1B for quality. Using smaller models significantly degrades audio quality.

**Q: How much GPU memory is needed?**
A: Minimum 20GB for reliable operation. 30GB recommended. Can use CPU but very slow (~2min per 20s audio).

**Q: Can multiple requests run in parallel?**
A: Currently no. Queue them. Each request runs sequentially due to GPU memory constraints. Future: implement request queuing.

---

## ğŸ“ Support

**Issues?** Check [TECHNICAL_DEVELOPER_GUIDE.md](../TECHNICAL_DEVELOPER_GUIDE.md)

**Errors?** Check server logs:
```bash
docker logs csm_container 2>&1 | tail -50
```

---

**Last Updated:** January 29, 2026  
**Maintainer:** AI/ML Team  
**Status:** Production Ready âœ…
